{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "import pandas as pd\n",
        "\n",
        "# Your Data Lake Gen 2 path\n",
        "WAREHOUSE_PATH = \"abfss://\"\n",
        "LANDING_FOLDER = \"landing\"\n",
        "BRONZE_FOLDER = \"bronze\"\n",
        "RAW_FILE_EXT = \"out\"\n",
        "PROCESSED_FILE_EXT = \"csv\"\n",
        "\n",
        "RAW_FILE_FOLDER = f\"{WAREHOUSE_PATH}/{LANDING_FOLDER}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from pandas.errors import EmptyDataError\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "\n",
        "def get_processed_path(raw_path: str) -> str:\n",
        "    \"\"\" Creates the processed file destination based on the raw file path \"\"\"\n",
        "    processed_path = raw_path.replace(LANDING_FOLDER, BRONZE_FOLDER) \\\n",
        "        .replace(RAW_FILE_EXT, PROCESSED_FILE_EXT)\n",
        "    assert raw_path != processed_path\n",
        "    return processed_path\n",
        "\n",
        "def file_already_exists(path: str) -> bool:\n",
        "    \"\"\" Checks if file already exists on the Datalake\"\"\"\n",
        "    return mssparkutils.fs.exists(path)\n",
        "\n",
        "def process_data(bronze_file_path: str) -> pd.DataFrame:\n",
        "    \"\"\" Loads the raw data and applies the processing at loading \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(bronze_file_path, sep=\"\\t\", skiprows=6,header=[0,1])\n",
        "    except EmptyDataError:\n",
        "        print(\"Empty file: \", bronze_file_path)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Dropping multilevel index for SQL compatibility\n",
        "    df.columns = [column[0] for column in df.columns]\n",
        "    df.reset_index(inplace=True)\n",
        "    return df\n",
        "\n",
        "def store_data(data: pd.DataFrame, file_path: str) -> None:\n",
        "    data.to_csv(file_path, sep=\",\")\n",
        "\n",
        "def retrieve_files_in_folders(folder_path: str, file_ext: str) -> list:\n",
        "    file_infos = files = mssparkutils.fs.ls(folder_path)\n",
        "\n",
        "    files = [file.path for file in file_infos if file.path.endswith(file_ext)]\n",
        "\n",
        "    return files\n",
        "\n",
        "\n",
        "def apply_data_processing(raw_data_path: str):\n",
        "    raw_files_list = retrieve_files_in_folders(raw_data_path, file_ext=RAW_FILE_EXT)\n",
        "    processed_files = []\n",
        "\n",
        "    for raw_file_path in raw_files_list:\n",
        "        processed_file_path = get_processed_path(raw_file_path)\n",
        "        if file_already_exists(processed_file_path):\n",
        "            continue\n",
        "        df = process_data(raw_file_path)\n",
        "        if not df.empty:\n",
        "            store_data(df, processed_file_path)\n",
        "            processed_files.append(processed_file_path)\n",
        "\n",
        "    return processed_files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(\"Processing files in: \", RAW_FILE_FOLDER)\n",
        "new_files = apply_data_processing(RAW_FILE_FOLDER)\n",
        "print(\"New files created: \", new_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# test_file_path = \"abfss://latticefs@latticedatalake.dfs.core.windows.net/synapse/workspaces/latticeworkspace/warehouse/landing/1.5MW-1682790356.out\"\n",
        "# df = process_data(test_file_path)\n",
        "\n",
        "# df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
