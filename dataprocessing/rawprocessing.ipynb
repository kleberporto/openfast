{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "WAREHOUSE_PATH = \"abfss://latticefs@latticedatalake.dfs.core.windows.net/synapse/workspaces/latticeworkspace/warehouse\"\r\n",
        "LANDING_FOLDER = \"landing\"\r\n",
        "BRONZE_FOLDER = \"bronze\"\r\n",
        "RAW_FILE_EXT = \"out\"\r\n",
        "PROCESSED_FILE_EXT = \"csv\"\r\n",
        "\r\n",
        "RAW_FILE_FOLDER = f\"{WAREHOUSE_PATH}/{LANDING_FOLDER}/\""
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "from pandas.errors import EmptyDataError\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "\r\n",
        "def get_processed_path(raw_path: str) -> str:\r\n",
        "    \"\"\" Creates the processed file destination based on the raw file path \"\"\"\r\n",
        "    processed_path = raw_path.replace(LANDING_FOLDER, BRONZE_FOLDER) \\\r\n",
        "        .replace(RAW_FILE_EXT, PROCESSED_FILE_EXT)\r\n",
        "    assert raw_path != processed_path\r\n",
        "    return processed_path\r\n",
        "\r\n",
        "def file_already_exists(path: str) -> bool:\r\n",
        "    \"\"\" Checks if file already exists on the Datalake\"\"\"\r\n",
        "    return mssparkutils.fs.exists(path)\r\n",
        "\r\n",
        "def process_data(bronze_file_path: str) -> pd.DataFrame:\r\n",
        "    \"\"\" Loads the raw data and applies the processing at loading \"\"\"\r\n",
        "    try:\r\n",
        "        df = pd.read_csv(bronze_file_path, sep=\"\\t\", skiprows=6,header=[0,1])\r\n",
        "    except EmptyDataError:\r\n",
        "        print(\"Empty file: \", bronze_file_path)\r\n",
        "        return pd.DataFrame()\r\n",
        "\r\n",
        "    # Dropping multilevel index for SQL compatibility\r\n",
        "    df.columns = [column[0] for column in df.columns]\r\n",
        "    df.reset_index(inplace=True)\r\n",
        "    return df\r\n",
        "\r\n",
        "def store_data(data: pd.DataFrame, file_path: str) -> None:\r\n",
        "    data.to_csv(file_path, sep=\",\")\r\n",
        "\r\n",
        "def retrieve_files_in_folders(folder_path: str, file_ext: str) -> list:\r\n",
        "    file_infos = files = mssparkutils.fs.ls(folder_path)\r\n",
        "\r\n",
        "    files = [file.path for file in file_infos if file.path.endswith(file_ext)]\r\n",
        "\r\n",
        "    return files\r\n",
        "\r\n",
        "\r\n",
        "def apply_data_processing(raw_data_path: str):\r\n",
        "    raw_files_list = retrieve_files_in_folders(raw_data_path, file_ext=RAW_FILE_EXT)\r\n",
        "    processed_files = []\r\n",
        "\r\n",
        "    for raw_file_path in raw_files_list:\r\n",
        "        processed_file_path = get_processed_path(raw_file_path)\r\n",
        "        if file_already_exists(processed_file_path):\r\n",
        "            continue\r\n",
        "        df = process_data(raw_file_path)\r\n",
        "        if not df.empty:\r\n",
        "            store_data(df, processed_file_path)\r\n",
        "            processed_files.append(processed_file_path)\r\n",
        "\r\n",
        "    return processed_files\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing files in: \", RAW_FILE_FOLDER)\r\n",
        "new_files = apply_data_processing(RAW_FILE_FOLDER)\r\n",
        "print(\"New files created: \", new_files)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_file_path = \"abfss://latticefs@latticedatalake.dfs.core.windows.net/synapse/workspaces/latticeworkspace/warehouse/landing/1.5MW-1682790356.out\"\r\n",
        "# df = process_data(test_file_path)\r\n",
        "\r\n",
        "# df.head()\r\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}